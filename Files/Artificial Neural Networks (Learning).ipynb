{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks: Learning\n",
    "\n",
    "## The Learning Process\n",
    "\n",
    "When intializing the _Neural Network_ model, the weights will be assigned small random values close to 0, because the model won't know how the input variables affect the output. Once intialized, each neuron will learn by:\n",
    "\n",
    "* Calculating the prediction value $\\hat y$ using an assigned activation function. This is known as __Forward Propgation__.\n",
    "\n",
    "<img src=\"cost.png\" width=\"700px;\" alt=\"Flow from input layer to output layer.\" />\n",
    "\n",
    "* Comparing the prediction value to the actual value, and calculate the difference through a cost function.\n",
    "\n",
    "$$\\large C = \\frac{(\\hat y - y)^2}{2} $$\n",
    "\n",
    "* Using the cost function and modifying the synapse weights to reduce the cost value. This is known as __Backpropogation__. The neurons that reduce the cost value will be have a greater weighted synapse, while the neurons that increase the cost value will have a lower weighted synapse. \n",
    "\n",
    "<img src=\"cost-back.png\" width=\"700px;\" alt=\"Backpropogation\" />\n",
    "\n",
    "The model will perform the steps above for $n$ number of epochs to reduce the value from the cost function. One epoch is one complete presentation of the training data, meaning the model has learned from the entire training dataset once. Reaching a cost of 0 is not necessarily the end goal, as overfitting may occur.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "<img src=\"gradientDescent.png\" width=\"600px;\" alt=\"Moving in the direction of descent to minimize the cost function.\" />\n",
    "\n",
    "_Gradient Descent_ is an optimization algorithm used to minimize the value of some function by moving in the direction of \"steepest descent\". It is a very efficient solution to figuring out better weight values in a neural network and minimizing the value of the cost function, especially because brute force solutions would take years to compute for most _Neural Network_ models with today's computational power.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "<img src=\"batch.png\" width=\"700px;\" alt=\"Entire dataset is used to calculate the cost value.\" />\n",
    "\n",
    "_Batch Gradient Descent_ is an application of gradient descent, where the cost function would be calculated using one entire epoch. From the general value of this cost function, the weights are all correspondingly changed. It is effective when dealing with convex cost functions like the one seen above and is a deterministic algorithm. The cost function for batch gradient descent is:\n",
    "\n",
    "$$\\LARGE C = \\sum_{d = 1}^{D}{\\large \\phi(\\sum_{i = 1}^{I} (x_i \\times w_i))}$$\n",
    "\n",
    "__Where:__\n",
    "\n",
    "* __C__: Cost Value\n",
    "* __d__: Current Datapoint\n",
    "* __D__: Total # of Datapoints\n",
    "* __i__: Current Input\n",
    "* __I__: Total # of Inputs\n",
    "* __x<sub>i</sub>__: Input Value\n",
    "* __w<sub>i</sub>__: Corresponding Weight Value\n",
    "\n",
    "### Sochastic Gradient Descent\n",
    "\n",
    "<img src=\"invalid.png\" width=\"700px;\" alt=\"Polynomial function that has been wrongly predicted due to Batch Gradient Descent\" />\n",
    "\n",
    "There are two cases where _Batch Gradient Descent_ isn't very useful. One is when a polynomial cost function is used, while the other is when the 3<sup>rd</sup> dimension or more is considered due to more then 2 input parameters; the latter is very likely to happen. The problem with these cases is that they cause many minimums within the graph, so a algorithm that always travels in the direction of \"steepest descent\" may reach a low cost value that isn't ideal. \n",
    "\n",
    "<img src=\"sochastic.png\" width=\"700px;\" alt=\"The neural network learns from each datapoint\" />\n",
    "\n",
    "The _Sochastic Gradient Descent_ algorithm fixes the issues faced by _Batch Gradient Descent_ by creating a cost value and performing backpropogation for each datapoint in random order. Instead of using all the datapoints at once to perform backpropogation, the _Neural Network_ learns from each datapoint. This will not only be faster because only one datapoint needs to be loaded into memory at a time, but will also have a better chance of finding the ideal cost value because there will be higher fluctuations in the cost function due to the variance between each datapoint.  \n",
    "\n",
    "### Mini Batch Gradient Descent \n",
    "\n",
    "A combination of _Batch Gradient Descent_ and *Sochastic Gradient Descent*, _Mini Batch Gradient Descent_ creates a cost value from a randomly picked set of datapoints instead of just a single datapoint. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
