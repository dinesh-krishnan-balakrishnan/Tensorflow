{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Code\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "<img src=\"directories.png\" width=\"500px;\" alt=\"Dataset directory structure.\" />\n",
    "\n",
    "The _training_ and _test_ sets are separated into 2 different directories, which is necessary to help Python distinguish between the two sets. Within the _training_ and _test_ set directories, the images of the cats and dogs are also separated into their own directories. Python will automatically recognize the images in the different directories as being part of the two different classes and perform classification with this understanding.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Fixing Image Specificity\n",
    "\n",
    "The training and test set images also often depict the images of dogs and cats in specific positions, which may cause the model to overfit. An example of overfitting due to position is when images of dogs jumping are classified as dogs, while images of dogs sitting are classified as cats. _Image augmentation_ will help artifically create different perspectives of dogs and cats, which will help increase the ability of the _CNN_ model to generalize. A few ways to perform _Image Augmentation_ involve flipping images, zooming in and out of them, shearing them, as well as downsampling.\n",
    "\n",
    "* __Shearing:__\n",
    "<img src=\"shearing.png\" width=\"300px;\" alt=\"Example of an image being sheared.\" />\n",
    "* __Image Augmentation:__ Creating artificial variation within a dataset by altering image structure. <br>\n",
    "* __Dowsampling:__ The reduction in the resolution of an image while maintaining its 2D representation.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Code\n",
    "\n",
    "__Image Augmentation:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tf.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Augments images in the training set.\n",
    "train_datagen = ImageDataGenerator(\n",
    "        # Downsamples images to 1/255ths of their original size for faster computation.\n",
    "        rescale = 1./255,\n",
    "        # Range for shearing angle (Start: 0). Generator will pick a value from the range randomly.\n",
    "        shear_range = 0.2,\n",
    "        # Range for zoom value (Start: 0). Generator will pick a value from the range randomly.\n",
    "        zoom_range = 0.2,\n",
    "        # If set to True, randomly flips images across the vertical axis.\n",
    "        horizontal_flip = True)\n",
    "\n",
    "# Rescales images in the test set to match the image size of the training set.\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Creates the training set from the training set generator created.\n",
    "training_set = train_datagen.flow_from_directory (\n",
    "        # Path to directory containing the training set data.\n",
    "        'dataset/training_set',\n",
    "        # Downsamples images to be 64 x 64 pixels, which is expected by the convolutional layer in the CNN.\n",
    "        target_size = (64, 64),\n",
    "        # Model will update weights and feature detectors after sampling 32 elements.\n",
    "        batch_size = 32,\n",
    "        # There are two classes: cats and dogs.\n",
    "        class_mode ='binary'\n",
    ")\n",
    "\n",
    "# Creates the test set from the test set generator created.\n",
    "test_set = test_datagen.flow_from_directory (\n",
    "        'dataset/test_set',\n",
    "        target_size = (64, 64),\n",
    "        batch_size = 32,\n",
    "        class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "__Creating the CNN:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.keras.models import Sequential\n",
    "from tf.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten  \n",
    "\n",
    "# Creating the ANN.\n",
    "classifier = Sequential()\n",
    "\n",
    "# The convolutional layer consists of 32 feature detectors which are 3 x 3 pixels.\n",
    "# The convolutional layer accepts 64 x 64 pixel images. The value of 3 represents the RGB layers.\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# The pooling layer dowsamples the feature maps, transforming 4 pixels to 1 pixel.\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattens the pooled feature maps into an input layer for the ANN.\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Hidden Layer\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# Output Layer\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiles the CNN classifier.\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "__Fitting the Classifier & Testing Accuracy:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "8000/8000 [==============================] - 810s 101ms/step - loss: 0.4131 - accuracy: 0.8025 - val_loss: 0.6767 - val_accuracy: 0.7561\n",
      "Epoch 2/25\n",
      "8000/8000 [==============================] - 803s 100ms/step - loss: 0.1656 - accuracy: 0.9341 - val_loss: 1.6979 - val_accuracy: 0.7604\n",
      "Epoch 3/25\n",
      "8000/8000 [==============================] - 809s 101ms/step - loss: 0.0849 - accuracy: 0.9691 - val_loss: 1.0587 - val_accuracy: 0.7497\n",
      "Epoch 4/25\n",
      "8000/8000 [==============================] - 834s 104ms/step - loss: 0.0569 - accuracy: 0.9802 - val_loss: 1.0196 - val_accuracy: 0.7432\n",
      "Epoch 5/25\n",
      "8000/8000 [==============================] - 810s 101ms/step - loss: 0.0449 - accuracy: 0.9844 - val_loss: 1.2946 - val_accuracy: 0.7488\n",
      "Epoch 6/25\n",
      "8000/8000 [==============================] - 842s 105ms/step - loss: 0.0379 - accuracy: 0.9877 - val_loss: 2.1530 - val_accuracy: 0.7523\n",
      "Epoch 7/25\n",
      "8000/8000 [==============================] - 869s 109ms/step - loss: 0.0313 - accuracy: 0.9898 - val_loss: 3.2400 - val_accuracy: 0.7568\n",
      "Epoch 8/25\n",
      "8000/8000 [==============================] - 865s 108ms/step - loss: 0.0278 - accuracy: 0.9911 - val_loss: 2.4652 - val_accuracy: 0.7581\n",
      "Epoch 9/25\n",
      "8000/8000 [==============================] - 863s 108ms/step - loss: 0.0251 - accuracy: 0.9920 - val_loss: 1.7136 - val_accuracy: 0.7623\n",
      "Epoch 10/25\n",
      "8000/8000 [==============================] - 839s 105ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 2.0319 - val_accuracy: 0.7460\n",
      "Epoch 11/25\n",
      "8000/8000 [==============================] - 880s 110ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 3.0225 - val_accuracy: 0.7554\n",
      "Epoch 12/25\n",
      "8000/8000 [==============================] - 874s 109ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 1.2855 - val_accuracy: 0.7714\n",
      "Epoch 13/25\n",
      "8000/8000 [==============================] - 871s 109ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 3.4726 - val_accuracy: 0.7628\n",
      "Epoch 14/25\n",
      "8000/8000 [==============================] - 880s 110ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 2.0425 - val_accuracy: 0.7601\n",
      "Epoch 15/25\n",
      "8000/8000 [==============================] - 849s 106ms/step - loss: 0.0147 - accuracy: 0.9952 - val_loss: 2.3025 - val_accuracy: 0.7520\n",
      "Epoch 16/25\n",
      "8000/8000 [==============================] - 859s 107ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 3.2641 - val_accuracy: 0.7572\n",
      "Epoch 17/25\n",
      "8000/8000 [==============================] - 864s 108ms/step - loss: 0.0129 - accuracy: 0.9960 - val_loss: 0.4356 - val_accuracy: 0.7550\n",
      "Epoch 18/25\n",
      "8000/8000 [==============================] - 845s 106ms/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 2.3935 - val_accuracy: 0.7531\n",
      "Epoch 19/25\n",
      "8000/8000 [==============================] - 866s 108ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.5179 - val_accuracy: 0.7559\n",
      "Epoch 20/25\n",
      "8000/8000 [==============================] - 868s 108ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 4.0903 - val_accuracy: 0.7602\n",
      "Epoch 21/25\n",
      "8000/8000 [==============================] - 871s 109ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 2.6810 - val_accuracy: 0.7590\n",
      "Epoch 22/25\n",
      "8000/8000 [==============================] - 869s 109ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 2.0875 - val_accuracy: 0.7642\n",
      "Epoch 23/25\n",
      "8000/8000 [==============================] - 872s 109ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.8172 - val_accuracy: 0.7489\n",
      "Epoch 24/25\n",
      "8000/8000 [==============================] - 869s 109ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 4.4512 - val_accuracy: 0.7590\n",
      "Epoch 25/25\n",
      "8000/8000 [==============================] - 873s 109ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 3.4104 - val_accuracy: 0.7581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26f20762288>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trains the model based on the images in the training set.\n",
    "classifier.fit_generator(\n",
    "        # Training data for the model.\n",
    "        training_set,\n",
    "        # Indicates the number of images to parse through in a single epoch. Usually equal to:\n",
    "        # Dataset_Size/Batch_Size\n",
    "        steps_per_epoch = 250,\n",
    "        # Number of epochs to perform when training.\n",
    "        epochs = 25,\n",
    "        \n",
    "        # Test data to validate the model's correctness.\n",
    "        validation_data = test_set,\n",
    "        # Number of images used to validate the correctness. Usually equal to the number of images in the\n",
    "        # test set, which is equal to 2000 in this case.\n",
    "        validation_steps = 2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results:__<br>\n",
    "The model is overfit on the training data, as it has an accuracy of almost 100% on the training set data but only 76% on the test set data. Parameter turning, an extra hidden layer, or an extra convolutional layer may be able to help find more intricate detail differences between a cat and a dog. In return, it will help prevent the drastic difference in accuracy between the training and test sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
